# FBDP project1

@张竞艺 151278047 2017-11-14

[toc]


##需求
- 需求1：针对股票新闻数据集中的新闻标题，编写WordCount程序，统计所有除Stop-word（如“的”，“得”，“在”等）出现次数k次以上的单词计数，最后的结果按照词频从高到低排序输出。
- 需求2：针对股票新闻数据集，以新闻标题中的词组为key，编写带URL属性的文档倒排索引程序，将结果输出到指定文件。

##主要设计思路
###需求1
需求1可具体拆解成如下几个步骤：
1. 批量读取文件提取新闻标题
2. 将新闻标题分词
3. 对分词后的结果进行词频统计
4. 根据词频降序输出

文件流设计如下:
![](media/15104762571184/15105688319480.jpg)

###需求2
需求2可具体拆解成如下步骤：
1. 在WordCount.java中获取股票新闻标题分词+url
2. 在InvertedIndexer.java中输出词组+url

文件流设计如下：
![](media/15104762571184/15105728858700.jpg)
考虑到本数据集数据较大，一个词组会输出上万个url的情况，为了更加方便直观的查看词组对应的每个url，故将日期也同步以升序输出。

##算法设计
###需求1
需求1对于传统WordCount的改进是**分词**和**降序输出**，其中核心部分是**降序输出**。

用一个并行计算任务无法同时完成单词词频统计和排序的，可以利用 Hadoop 的任务管道能力，用上一个任务`(词频统计) `的输出做为下一个任务`(排序) `的输入，顺序执行两个并行计算任务。

MapReduce 会把中间结果根据 key 排序并按 key 切成n份交给n个 Reduce 函数，Reduce 函数在处理中间结果之前也会有一个按 key 进行升序排序的过程，故 MapReduce 输出的最终结果实际上已经按 key 排好序。

传统的WordCount输出是将 `<词组，频次>` 作为 `<key,value>` 对，然后MapReduce默认根据 `key` 值升序输出，为了实现按词频**降序**排序，这里使用hadoop内置InverseMapper 类作为排序任务的 Mapper 类 `sortjob.setMapperClass(InverseMapper.class)` ，这个类的 map 函数将输入的 key 和 value 互换后作为中间结果输出，即将词频作为 key, 单词作为 value 输出, 然后得到按照词频升序排序的结果。

接下来需要解决将升序改为降序的问题。

此处可以利用hadoop内置比较类`Class WritableComparator`实现一个降序排序函数，官方API截图如下：

![](media/15104762571184/15105769316218.jpg)


###需求2
需求2本质是文档倒排索引，与传统的文档倒排索引不同的地方在于本程序并非以文档名称为索引，而是以新闻标题对应的url为索引，另外为了更加直观，本程序还对应输出了每个url对应的日期。

一个倒排作引由大量的posting列表组成，每一个posting列表和一个词组相关联，每个posting表示对应词组在一个文档的payload信息，包括URL、词频和新闻日期。

Mapper将 `<词组#url#日期，词频`> 作为输出的`\<key,value>`对，然后使用Combiner将Mapper的输出结果中value部分的词频进行统计；接着自定义 `HashPartitioner`，把组合的主键临时拆开，使得Partitioner单纯按照词组进行分区选择正确的Reduce节点，即将传入的key按照`#`进行分割出词组，使得 `<词组#url#日期，词频>`格式的key值只按照词组分发给Reducer，这样可保证同一个词组下的键值对一定被分到同一个Reduce节点。

Reducer从Partitioner得到键值对后，key值被进一步分割为词组、url和日期，由于Reduce自动按照key值升序排序，为了实现按照日期升序排序，故将url和日期的位置进行调换，即变成`日期 url`的形式，便可自动升序排序。




##程序和各个类的设计说明
###需求1
####1.批量读取文件

本程序所用数据集：
>某门户网站财经板块股票新闻数据集：download_data.zip
>    1.1 内容：收集沪市和深市若干支股票在某时间段内的若干条财经新闻标题
    1.2 格式：文件名：股票代号+股票名.txt；文件内容：股票代码+时间+新闻标题+网页URL（以空格分隔）

首先用hadoop遍历文件夹的内置方法`iteratorPath`遍历给定数据集并对每个txt文件进行操作分词操作并输出两个文件：

- 用于作为下一步输入的 `segment.txt`（标题）
- 用于需求2文档倒排的`titles.txt`（标题+url+日期）

####2.分词
[Java分布式中文分词组件](https://github.com/ysc/word)
>Java分布式中文分词组件 - word分词
>word分词是一个Java实现的分布式的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。能通过自定义配置文件来改变组件行为，能自定义用户词库、自动检测词库变化、支持大规模分布式环境，能灵活指定多种分词算法，能使用refine功能灵活控制分词结果，还能使用词频统计、词性标注、同义标注、反义标注、拼音标注等功能。提供了10种分词算法，还提供了10种文本相似度算法，同时还无缝和Lucene、Solr、ElasticSearch、Luke集成。注意：word1.3需要JDK1.8

下载API然后在project中引入jar包即可直接在程序中使用

为了避免多余的文件操作，本程序在提取新闻标题后写入txt文件前进行分词操作，可以输出分词结果。即先分词后输出。
相关代码如下：

![](media/15104762571184/15105782537290.jpg)
####3.词频统计WordCount
#####mapper类
这个类实现 Mapper 接口中的 map 方法，输入参数中的 value 是文本文件中的一行，利用 `StringTokenizer` 将这个字符串拆成单词，然后将输出结果 <词组,1> 写入到 `org.apache.hadoop.io.Text` 中。
相关代码：

```
public void map(Object key, Text value, Context context)
                 throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString()," ,.\":\t\n");
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken().toLowerCase());
                context.write(word, one);
            }
        }
```
#####reducer类
这个类实现 Reducer 接口中的 reduce 方法, 输入参数中的 key, values 是由 Map 任务输出的中间结果，values 是一个 Iterator, 遍历这个 Iterator, 就可以得到属于同一个 key 的所有 value。在本程序中key 是一个单词，value 是词频。只需要将所有的 value 相加，就可以得到这个单词的总的出现次数。
相关代码：

```
public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) 
                sum += val.get();
            result.set(sum);
            if(sum >= min_frequency && sum <= max_frequency )  
                context.write(key, result);
        }
```
####4.根据词频降序
用hadoop内置类`IntWritable.Comparator`实现一个函数`IntWritableDecreasingComparator` 对key进行比较并降序输出
相关代码：

```
private static class IntWritableDecreasingComparator extends IntWritable.Comparator {
        //hadoop内置比较类
        public int compare(WritableComparable a, WritableComparable b) {
            return -super.compare(a, b);
        }
        public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
            return -super.compare(b1, s1, l1, b2, s2, l2);
        }
    }
```
###需求2
####1.map方法
`map( )`函数使用自定义的FileNameRecordReader，将词组、url和日期以`#`作为分隔符，并将` 词组#url#日期`整体作为key，频次作为value输出键值对。
相关代码:

```
protected void map(Text key, Text value, Context context)
                throws IOException, InterruptedException {
            // map()函数这里使用自定义的FileNameRecordReader
            String line = value.toString().toLowerCase();
            StringTokenizer itr = new StringTokenizer(line,"\n");
            for (; itr.hasMoreTokens();) {
                String[] itr1 = itr.nextToken().split(" ");
                int l = itr1.length;
                for(int i = 0;i<l-1;i++){
                    Text word = new Text();
                    word.set(itr1[i]+"#" +itr1[l-1]+" #"+itr1[l-2]);   //词组#url#日期
                    context.write(word, new IntWritable(1));
                }
            }
        }
```
####2.Combiner类
Hadoop用过在Mapper类结束后、传入Reduce节点之前用一个Combiner类来解决相同主键键值对的合并处理。Combiner类主要作用是为了合并和减少Mapper的输出从而减少Reduce节点的负载。

本程序使用Combiner将Mapper的输出结果中value部分的词频进行统计。
相关代码片段:

```
int sum = 0;
for (IntWritable val : values)
    sum += val.get();
result.set(sum);
context.write(key, result);
```
####3.Partitioner类
由于一个Reduce节点所处理的数据可能会来自多个Map节点，因此为了避免在Reduce计算过程中不同Reduce节点间存在数据相关性，需要一个Partitioner的过程。Partitioner用来控制Map输出的中间结果键值对的划分，分区总数与作业的Reduce任务的数量一致。

本程序自定义一个HashPartitioner类，先继承Partitioner类，并重载getPartition( )方法。 getPartition( )方法返回一个0到Reducer数目之间的整型值来确定将`<key,value>`送到哪一个Reducer中，它的参数除了key和value之外还有一个numReduceTasks表示总的划分的个数。

HashPartitioner把组合的主键临时拆开，使得Partitioner将传入的key按照`#`进行分割出词组，只按照词组进行分区选择正确的Reduce节点，这样可保证同一个词组下的键值对一定被分到同一个Reduce节点。

相关代码：

```
public static class NewPartitioner extends HashPartitioner<Text, IntWritable> {
        public int getPartition(Text key, IntWritable value, int numReduceTasks) {
            String term = key.toString().split("#")[0]; // <term#docid> => term
            return super.getPartition(new Text(term), value, numReduceTasks);
        }
    }
```
####4.Reducer类
Reduce( )方法主要实现以下功能：

- 将key按照`#`进行分割
- 交换原key中url和日期的顺序
- 针对每个词组输出对应的多个url和日期
- 对词组出现的次数进行计数
- 筛选一定频次的词组并输出


##程序运行和实验结果说明和分析
###需求1
####程序运行说明

`wordcount.java` 需求是将大于k次以上的词组以降序输出，本程序对需求做了进一步改进，能够具体输出一个频次段的词组，共设置4个参数，分别为 `<文件输入路径>`  `<输出结果路径>`  `<频次下限>`  `<频次上限> `
此处`<files input path>`是指WordCount的输入路径，即分词的输出文件，此处只需提供一个空文件夹即可

####实验结果截图
以输出频次在4500~6500的词组为例运行程序：
输入参数:
![](media/15106488009278/15106661766831.jpg)
运行结果：
![](media/15106488009278/15106661518225.jpg)


###需求2
####程序运行说明
本程序对需求做了进一步改进，能够具体索引一个频次段的词组的url，共设置4个参数，分别为 `<文件输入路径>`  `<输出结果路径>`  `<频次下限>`  `<频次上限> `
此处`<files input path>`是指新闻标题分词后带有url和日期的文件，形式如下：
![](media/15106488009278/15106818665017.jpg)
需要注意的一点是由于hdfs文件路径的限制，数据集的路径直接在程序中给出而非作为参数给出：
![](media/15106488009278/15106820010397.jpg)


####实验结果截图
由于输出html太多不方便查看结果，取频次15~29次词组运行本程序
输入参数：
![](media/15106488009278/15106808386163.jpg)

![](media/15106488009278/15106777205172.jpg)
![](media/15106488009278/15106808228391.jpg)

##创新点
###需求1
需求1是将股票新闻标题中出现k次以上的单词按照词频降序输出，由于考虑到频次太低和频次太高的词组对股票数据分析无太大意义，故本人将程序进一步优化，使得能够输出a~b之间的一段词频而非只是k次以上的词组。

| 频次最高的部分词组 | 频次最低的部分词组 |
| --- | --- |
|  <div>![](media/15106488009278/15106568841716.jpg)</div> |  ![](media/15106488009278/15106570257100.jpg)|


###需求2
- 为了使数据更有意义，在新闻标题url输出的同时同步输出对应日期
- 为了方便查看，每个词组对应的url按照日期从旧到新的顺序输出
![](media/15106488009278/15106800313640.jpg)

##存在的不足和可能的改进之处
- word分词器由于分词精确度较高、功能较为复杂的原因而运行时较慢，可用的解决方案是在对除了分词之外的其他功能无要求、分词难度不大的情况下可以考虑用其他可替代的轻量中文分词器
- 目前由于分词器的问题，仍会出现 一些奇奇怪怪的问题比如最终结果有一部分只输出标题不输出url等，解决方案是换个分词器……
- 需求1可改进之处：想办法啊把数据集的路径作为参数输入而非在程序中固定
- 需求2：可把词组检索结果按照日期降序输出，即时间上由新到旧的顺序
- 需求2：当词组出现的次数太大时，可设置一种排序机制只输出排序前50个url







